{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Based Next Word Prediction Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79171 sentences\n",
      "Parsed 50000 sentences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "no_word = 'NO_WORD'\n",
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split comments into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences\"%(len(sentences))\n",
    "sentences = sentences[0:50000]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082845 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49713 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'textbox' and appeared 5 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "vocab.insert(0,(u'NO_WORD',1000))\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating each sentence in the collection to be 10 words long\n",
    "## I have split the sentences unknowingly thus making sentence incomplete\n",
    "## This methos needs to be checked\n",
    "text,next_word,maxlen = [],[],\n",
    "for sent in tokenized_words:\n",
    "    if len(sent) < maxlen:\n",
    "        continue\n",
    "    if len(sent) >= maxlen:\n",
    "        val = sent[0:maxlen-1]\n",
    "        val.append(sentence_end_token)\n",
    "        text.append(val)\n",
    "        next_word.append(sent[maxlen-1])\n",
    "print text[0:10]\n",
    "print \"--------------------\"\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(text),maxlen,vocabulary_size),dtype = np.bool)\n",
    "y = np.zeros((len(text),vocabulary_size),dtype = np.bool)\n",
    "for i,sent in enumerate(text):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model needs to be trained on a GPU\n",
    "## The output needs to be predicted by using a mixture of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Masking Rather Than To Slice The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_word = []\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        next_word.append(string[-2])\n",
    "        l = [no_word for i in range(maxlen - len(string) + 1)]\n",
    "        string.remove(sentence_end_token)\n",
    "        string[-1] = sentence_end_token\n",
    "        string = l + string\n",
    "    else:\n",
    "        string = string[0:maxlen]\n",
    "        next_word.append(string.pop(maxlen-1))\n",
    "        string.append(sentence_end_token)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenized_words):\n",
    "    if len(sent) <= 3:\n",
    "        tokenized_words.pop(i)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tokenized_words[0:10]\n",
    "print '--------------------------------------------'\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Frequency distribution of the next word\n",
    "dist = nltk.FreqDist(next_word)\n",
    "for key in sorted(dist,key=dist.get,reverse=True)[0:10]:\n",
    "    print str(key.encode('utf-8')) + \" : \" + str(dist[unicode(key)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## The above model seem to have not captured the essence of long words\n",
    "## So we use the sliding window approach\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        l = [no_word for i in range(maxlen - len(string))]\n",
    "        string = l + string\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_words)):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "for string in tokenized_words:\n",
    "    if len(string) < 10:\n",
    "        print \"Warning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Converting the segregated sentences into one sentence (text)\n",
    "text = []\n",
    "for string in tokenized_words:\n",
    "    for word in string:\n",
    "        text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNKNOWN_TOKEN 59346\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for word in text:\n",
    "    try:\n",
    "        dic[word] += 1\n",
    "    except:\n",
    "        dic[word] = 1\n",
    "print dic.keys()[dic.values().index(max(dic.values()))],max(dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Making a sentence matrix having sentences of length 'maxlen'\n",
    "sentences,next_word = [],[]\n",
    "step = 3\n",
    "for i in range(0,len(text) - maxlen,step):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_word.append(text[i+maxlen])\n",
    "sentences = sentences[0:100000]\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = nltk.FreqDist(next_word)\n",
    "for key in sorted(dist,key=dist.get,reverse=True)[0:10]:\n",
    "    print str(key.encode('utf-8')) + \" : \" + str(dist[unicode(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sanity Check\n",
    "for string in sentences:\n",
    "    if len(string) != 10:\n",
    "        print \"Warning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(sentences),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "y = np.zeros((len(sentences),vocabulary_size+1),dtype = np.bool)\n",
    "for i,sent in enumerate(sentences):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size+1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = 'Are you going out with him to the'\n",
    "def convert(string):\n",
    "    word = nltk.word_tokenize(string)\n",
    "    word.insert(0,sentence_start_token)\n",
    "    word.append(sentence_end_token)\n",
    "    word = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in word]\n",
    "    return word\n",
    "print convert(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word,i = convert(string),0\n",
    "X_test = np.zeros((len(word),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "for t,w in enumerate(word):\n",
    "    X_test[i,t,w] = 1\n",
    "    i += 1\n",
    "print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    yPred = model.predict_classes(X_test,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print yPred\n",
    "print yPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in yPred:\n",
    "    print index_to_word[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
