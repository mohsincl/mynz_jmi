{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Based Next Word Prediction Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79171 sentences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "no_word = 'NO_WORD'\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split comments into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1716192 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65751 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999\n",
      "8000\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "print len(vocab)\n",
    "vocab.insert(0,(u'NO_WORD',100000))\n",
    "print len(vocab)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'NO_WORD', 100000) (u'devoted', 10)\n"
     ]
    }
   ],
   "source": [
    "len(vocab)\n",
    "print vocab[0],vocab[7999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print max(word_to_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating each sentence in the collection to be 10 words long\n",
    "## I have split the sentences unknowingly thus making sentence incomplete\n",
    "## This methos needs to be checked\n",
    "text,next_word,maxlen = [],[],\n",
    "for sent in tokenized_words:\n",
    "    if len(sent) < maxlen:\n",
    "        continue\n",
    "    if len(sent) >= maxlen:\n",
    "        val = sent[0:maxlen-1]\n",
    "        val.append(sentence_end_token)\n",
    "        text.append(val)\n",
    "        next_word.append(sent[maxlen-1])\n",
    "print text[0:10]\n",
    "print \"--------------------\"\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(text),maxlen,vocabulary_size),dtype = np.bool)\n",
    "y = np.zeros((len(text),vocabulary_size),dtype = np.bool)\n",
    "for i,sent in enumerate(text):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model needs to be trained on a GPU\n",
    "## The output needs to be predicted by using a mixture of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Masking Rather Than To Slice The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'body', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', u'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'ppr', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', u',', 'UNKNOWN_TOKEN', u'points', u'per', u'completion', u',', u'6', u'points', u'per', u'td', u'thrown', u',', u'and', u'some', u'bonuses', u'for', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', u'that', u'qb', u'has', u'the', u'highest', u'potential', u'for', u'points', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', u'qb', u'in', u'the', u'first', u'round', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', u'not', u'run', u'the', u'mandatory', u'background', u'check', u'on', u'the', u'buyer', u'and', u'still', u'sell', u'the', u'gun', u'to', u'the', u'felon', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', u'the', u'gun', u'to', u'them', u'when', u'they', u'see', u'they', u\"'re\", u'a', u'felon', u'on', u'the', u'background', u'check', u'.', u'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "## Run above 4 Scripts to run the script below\n",
    "print tokenized_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_word = []\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        next_word.append(string[-2])\n",
    "        l = [no_word for i in range(maxlen - len(string) + 1)]\n",
    "        string.remove(sentence_end_token)\n",
    "        string[-1] = sentence_end_token\n",
    "        string = l + string\n",
    "    else:\n",
    "        string = string[0:maxlen]\n",
    "        next_word.append(string.pop(maxlen-1))\n",
    "        string.append(sentence_end_token)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenized_words):\n",
    "    if len(sent) <= 3:\n",
    "        tokenized_words.pop(i)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', 'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', u'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'ppr', 'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', 'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', 'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', 'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', 'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', 'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', 'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', 'SENTENCE_END'], [u'SENTENCE_START', u'a', u'dishonest', u'seller', u'is', u\"n't\", u'going', u'to', u'run', 'SENTENCE_END']]\n",
      "--------------------------------------------\n",
      "[u'they', u'.', u',', u'that', u'site', u'qb', u'not', u'SENTENCE_END', u'the', u'the']\n"
     ]
    }
   ],
   "source": [
    "print tokenized_words[0:10]\n",
    "print '--------------------------------------------'\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the word: 10\n"
     ]
    }
   ],
   "source": [
    "## Checking for the length of each word\n",
    "print 'Maximum length of the word: '+str(maxlen)\n",
    "for word in tokenized_words:\n",
    "    if len(word) != 10:\n",
    "        print len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(tokenized_words),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "y = np.zeros((len(tokenized_words),vocabulary_size+1),dtype = np.bool)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]]\n",
      "---------------------------------------------------\n",
      "[[False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78526, 10, 8001)\n",
      "(78526, 8001)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size+1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8000, 14, 8, 1338, 204, 16, 125, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "string = 'This is a beautiful day for going and'\n",
    "def convert(string):\n",
    "    word = nltk.word_tokenize(string)\n",
    "    word.insert(0,sentence_start_token)\n",
    "    word.append(sentence_end_token)\n",
    "    word = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in word]\n",
    "    return word\n",
    "print convert(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False  True]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " ..., \n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]]\n"
     ]
    }
   ],
   "source": [
    "word = convert(string)\n",
    "X_test = np.zeros((len(word),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "for t,w in enumerate(word):\n",
    "    X_test[i,t,w] = 1\n",
    "print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    yPred = model.predict_classes(X_test,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3 3 3 3 3 3 3 3]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print yPred\n",
    "print yPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_END\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in yPred:\n",
    "    print index_to_word[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
