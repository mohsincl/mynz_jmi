{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Neuro Probabilistic Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79171 sentences\n"
     ]
    }
   ],
   "source": [
    "## Splitting the tweets into proper sentences and appending the start and end token\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split tweets into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1716192 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65751 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_words])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    def __init__(self,word_dim,hidden_dim = 100,bptt_truncate = 4):\n",
    "        ## Assigning the instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        ## Assigning the random values to the parameter\n",
    "        ## Values are assigned uniformly\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim),np.sqrt(1./word_dim),(hidden_dim,word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim),np.sqrt(1./hidden_dim),(word_dim,hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim),np.sqrt(1./hidden_dim),(hidden_dim,hidden_dim))\n",
    "    ## Implementing the forward propagation\n",
    "    def forward_propagation(self,x):\n",
    "        ## The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T+1,self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        ## Since output also needed to be saved at each time step\n",
    "        o = np.zeros((T,self.word_dim))\n",
    "        ## For each timestamp we use the Feed Forward Algorithm\n",
    "        for t in np.arange(T):\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o,s]\n",
    "    RNNNumpy.forward_propagation = forward_propagation\n",
    "    ## Function to predict the next word by choosing the maximum probability\n",
    "    def predict(self,x):\n",
    "        o,s = self.forward_propagation(x)\n",
    "        return np.argmax(o,axis = 1)\n",
    "    RNNNumpy.predict = predict\n",
    "    ## Function to calculate the loss function\n",
    "    ## Loss function gives information about how \"off\" we are\n",
    "    def calculate_total_loss(self,x,y):\n",
    "        L = 0\n",
    "        for i in np.arange(len(y)):\n",
    "            o,s = self.forward_propagation(x[i])\n",
    "            correct_word_prediction = o[np.arange(len(y[i])),y[i]]\n",
    "            L += -1*np.sum(np.log(correct_word_prediction))\n",
    "        return L\n",
    "    def calculate_loss(self,x,y):\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N\n",
    "    RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "    RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 8000)\n",
      "[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488\n",
      "   0.00012508]\n",
      " [ 0.00012448  0.00012615  0.00012402 ...,  0.00012514  0.00012425\n",
      "   0.00012528]\n",
      " [ 0.00012557  0.00012518  0.0001251  ...,  0.0001242   0.00012598\n",
      "   0.00012594]\n",
      " ..., \n",
      " [ 0.00012521  0.00012491  0.00012444 ...,  0.00012567  0.00012515\n",
      "   0.00012488]\n",
      " [ 0.00012482  0.00012524  0.0001249  ...,  0.00012436  0.00012405\n",
      "   0.00012565]\n",
      " [ 0.00012474  0.00012462  0.00012508 ...,  0.00012453  0.00012513\n",
      "   0.00012464]]\n",
      "(16,)\n",
      "[1284 6751 3936  238  774 6546 3037 6601 7299 6722 7008 2822 5786 1167 2147\n",
      " 5027]\n",
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987440\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print o.shape\n",
    "print o\n",
    "predictions = model.predict(X_train[10])\n",
    "print predictions.shape\n",
    "print predictions\n",
    "# Limit to 1000 examples to save time\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
