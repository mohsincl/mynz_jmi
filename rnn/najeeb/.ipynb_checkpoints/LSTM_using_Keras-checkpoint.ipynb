{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Based Next Word Prediction Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79171 sentences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "no_word = 'NO_WORD'\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split comments into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1716192 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65751 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999\n",
      "8000\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'devoted' and appeared 10 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "print len(vocab)\n",
    "vocab.insert(0,(u'NO_WORD',100000))\n",
    "print len(vocab)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'NO_WORD', 100000) (u'devoted', 10)\n"
     ]
    }
   ],
   "source": [
    "len(vocab)\n",
    "print vocab[0],vocab[7999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print max(word_to_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating each sentence in the collection to be 10 words long\n",
    "## I have split the sentences unknowingly thus making sentence incomplete\n",
    "## This methos needs to be checked\n",
    "text,next_word,maxlen = [],[],\n",
    "for sent in tokenized_words:\n",
    "    if len(sent) < maxlen:\n",
    "        continue\n",
    "    if len(sent) >= maxlen:\n",
    "        val = sent[0:maxlen-1]\n",
    "        val.append(sentence_end_token)\n",
    "        text.append(val)\n",
    "        next_word.append(sent[maxlen-1])\n",
    "print text[0:10]\n",
    "print \"--------------------\"\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(text),maxlen,vocabulary_size),dtype = np.bool)\n",
    "y = np.zeros((len(text),vocabulary_size),dtype = np.bool)\n",
    "for i,sent in enumerate(text):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model needs to be trained on a GPU\n",
    "## The output needs to be predicted by using a mixture of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Masking Rather Than To Slice The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'body', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', u'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'ppr', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', u',', 'UNKNOWN_TOKEN', u'points', u'per', u'completion', u',', u'6', u'points', u'per', u'td', u'thrown', u',', u'and', u'some', u'bonuses', u'for', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', u'that', u'qb', u'has', u'the', u'highest', u'potential', u'for', u'points', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', u'qb', u'in', u'the', u'first', u'round', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', u'not', u'run', u'the', u'mandatory', u'background', u'check', u'on', u'the', u'buyer', u'and', u'still', u'sell', u'the', u'gun', u'to', u'the', u'felon', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', u'the', u'gun', u'to', u'them', u'when', u'they', u'see', u'they', u\"'re\", u'a', u'felon', u'on', u'the', u'background', u'check', u'.', u'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "## Run above 4 Scripts to run the script below\n",
    "print tokenized_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_word = []\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        next_word.append(string[-2])\n",
    "        l = [no_word for i in range(maxlen - len(string) + 1)]\n",
    "        string.remove(sentence_end_token)\n",
    "        string[-1] = sentence_end_token\n",
    "        string = l + string\n",
    "    else:\n",
    "        string = string[0:maxlen]\n",
    "        next_word.append(string.pop(maxlen-1))\n",
    "        string.append(sentence_end_token)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenized_words):\n",
    "    if len(sent) <= 3:\n",
    "        tokenized_words.pop(i)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', 'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', u'ppr', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'ppr', 'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', 'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', 'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', 'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', 'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', 'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', 'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', 'SENTENCE_END'], [u'SENTENCE_START', u'a', u'dishonest', u'seller', u'is', u\"n't\", u'going', u'to', u'run', 'SENTENCE_END']]\n",
      "[u'they', u'.', u',', u'that', u'site', u'qb', u'not', u'SENTENCE_END', u'the', u'the']\n"
     ]
    }
   ],
   "source": [
    "print tokenized_words[0:10]\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the word: 10\n"
     ]
    }
   ],
   "source": [
    "## Checking for the length of each word\n",
    "print 'Maximum length of the word: '+str(maxlen)\n",
    "for word in tokenized_words:\n",
    "    if len(word) != 10:\n",
    "        print len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(tokenized_words),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "y = np.zeros((len(tokenized_words),vocabulary_size+1),dtype = np.bool)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]]\n",
      "---------------------------------------------------\n",
      "[[False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78526, 10, 8001)\n",
      "(78526, 8001)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size+1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1152/78526 [..............................] - ETA: 1351s - loss: 7.0833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-4ac90aac62c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
