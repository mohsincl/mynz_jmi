{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM Based Next Word Prediction Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79171 sentences\n",
      "Parsed 20000 sentences\n"
     ]
    }
   ],
   "source": [
    "maxlen = 10\n",
    "no_word = 'NO_WORD'\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "with open('reddit-comments-2015-08.csv') as f:\n",
    "    reader = csv.reader(f,skipinitialspace=True)\n",
    "    ## Split comments into sentences\n",
    "    sentences  = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "print \"Parsed %d sentences\"%(len(sentences))\n",
    "sentences = sentences[0:20000]\n",
    "print \"Parsed %d sentences\"%(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 433688 number of words\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the sentences into words\n",
    "count = 0\n",
    "tokenized_words = [nltk.word_tokenize(word) for word in sentences]\n",
    "for l in tokenized_words:\n",
    "    count += len(l)\n",
    "print \"Found %d number of words\"%(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28407 unique word tokens\n"
     ]
    }
   ],
   "source": [
    "## Counting the word frequencies in the word_tokenize\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_words))\n",
    "print \"Found %d unique word tokens\"%(len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7999\n",
      "8000\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'jumped' and appeared 3 times\n"
     ]
    }
   ],
   "source": [
    "## Hashing the most frequent words into the vocabulary\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "print len(vocab)\n",
    "vocab.insert(0,(u'NO_WORD',100000))\n",
    "print len(vocab)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times\"%(vocab[-1][0],vocab[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'NO_WORD', 100000) (u'jumped', 3)\n"
     ]
    }
   ],
   "source": [
    "len(vocab)\n",
    "print vocab[0],vocab[7999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    }
   ],
   "source": [
    "print max(word_to_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Creating each sentence in the collection to be 10 words long\n",
    "## I have split the sentences unknowingly thus making sentence incomplete\n",
    "## This methos needs to be checked\n",
    "text,next_word,maxlen = [],[],\n",
    "for sent in tokenized_words:\n",
    "    if len(sent) < maxlen:\n",
    "        continue\n",
    "    if len(sent) >= maxlen:\n",
    "        val = sent[0:maxlen-1]\n",
    "        val.append(sentence_end_token)\n",
    "        text.append(val)\n",
    "        next_word.append(sent[maxlen-1])\n",
    "print text[0:10]\n",
    "print \"--------------------\"\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(text),maxlen,vocabulary_size),dtype = np.bool)\n",
    "y = np.zeros((len(text),vocabulary_size),dtype = np.bool)\n",
    "for i,sent in enumerate(text):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model needs to be trained on a GPU\n",
    "## The output needs to be predicted by using a mixture of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Masking Rather Than To Slice The Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'body', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.2', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', u',', u'.2', u'points', u'per', u'completion', u',', u'6', u'points', u'per', u'td', u'thrown', u',', u'and', u'some', 'UNKNOWN_TOKEN', u'for', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', u'that', u'qb', u'has', u'the', u'highest', u'potential', u'for', u'points', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', u'site', u'and', u'noticed', u'that', u'top', u'qbs', u'had', u'300', u'points', u'more', u'than', u'the', u'top', 'UNKNOWN_TOKEN', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', u'qb', u'in', u'the', u'first', u'round', u'?', u'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', u'not', u'run', u'the', u'mandatory', u'background', u'check', u'on', u'the', u'buyer', u'and', u'still', u'sell', u'the', u'gun', u'to', u'the', u'felon', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', u'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', u'the', u'gun', u'to', u'them', u'when', u'they', u'see', u'they', u\"'re\", u'a', u'felon', u'on', u'the', u'background', u'check', u'.', u'SENTENCE_END']]\n"
     ]
    }
   ],
   "source": [
    "## Run above 4 Scripts to run the script below\n",
    "print tokenized_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_word = []\n",
    "def mask(string):\n",
    "    if len(string) < maxlen:\n",
    "        ## Pad the string with no_word type of string\n",
    "        next_word.append(string[-2])\n",
    "        l = [no_word for i in range(maxlen - len(string) + 1)]\n",
    "        string.remove(sentence_end_token)\n",
    "        string[-1] = sentence_end_token\n",
    "        string = l + string\n",
    "    else:\n",
    "        string = string[0:maxlen]\n",
    "        next_word.append(string.pop(maxlen-1))\n",
    "        string.append(sentence_end_token)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenized_words):\n",
    "    if len(sent) <= 3:\n",
    "        tokenized_words.pop(i)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    tokenized_words[i] = mask(tokenized_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', 'SENTENCE_END'], [u'SENTENCE_START', u'it', u\"'s\", u'a', u'slight', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', u'.2', 'UNKNOWN_TOKEN', 'SENTENCE_END'], [u'SENTENCE_START', u'standard', u'besides', u'1', u'points', u'for', u'15', u'yards', u'receiving', 'SENTENCE_END'], [u'SENTENCE_START', u'my', u'question', u'is', u',', u'is', u'it', u'wildly', u'clear', 'SENTENCE_END'], [u'SENTENCE_START', u'i', u'put', u'in', u'the', u'rules', u'at', u'a', u'ranking', 'SENTENCE_END'], [u'SENTENCE_START', u'would', u'it', u'be', u'dumb', u'not', u'to', u'grab', u'a', 'SENTENCE_END'], [u'SENTENCE_START', u'in', u'your', u'scenario', u',', u'a', u'person', u'could', u'just', 'SENTENCE_END'], [u'SENTENCE_START', u'there', u\"'s\", u'no', u'way', u'to', u'enforce', u'it', u'.', 'SENTENCE_END'], [u'SENTENCE_START', u'an', u'honest', u'seller', u'is', u'going', u'to', u'not', u'sell', 'SENTENCE_END'], [u'SENTENCE_START', u'a', u'dishonest', u'seller', u'is', u\"n't\", u'going', u'to', u'run', 'SENTENCE_END']]\n",
      "--------------------------------------------\n",
      "[u'they', u'.', u',', u'that', u'site', u'qb', u'not', u'SENTENCE_END', u'the', u'the']\n"
     ]
    }
   ],
   "source": [
    "print tokenized_words[0:10]\n",
    "print '--------------------------------------------'\n",
    "print next_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the word: 10\n"
     ]
    }
   ],
   "source": [
    "## Checking for the length of each word\n",
    "print 'Maximum length of the word: '+str(maxlen)\n",
    "for word in tokenized_words:\n",
    "    if len(word) != 10:\n",
    "        print len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Vectorizing each of the sentence into the matrix X\n",
    "## Matrix y contains the next word prediction for the whole sentence (LSTM)\n",
    "X = np.zeros((len(tokenized_words),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "y = np.zeros((len(tokenized_words),vocabulary_size+1),dtype = np.bool)\n",
    "for i,sent in enumerate(tokenized_words):\n",
    "    for t,word in enumerate(sent):\n",
    "        X[i,t,word_to_index[word]] = 1\n",
    "    y[i,word_to_index[next_word[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]]\n",
      "---------------------------------------------------\n",
      "[[False False False ..., False False False]]\n"
     ]
    }
   ],
   "source": [
    "print X[0:1]\n",
    "print \"---------------------------------------------------\"\n",
    "print y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19850, 10, 8001)\n",
      "(19850, 8001)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/karpathy/.theano/compiledir_Linux-3.19--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen,vocabulary_size+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(vocabulary_size+1))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_5 (LSTM)                    (None, 10, 512)       17436672    lstm_input_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 10, 512)       0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 512)           2099200     dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 512)           0           lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 8001)          4104513     dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8001)          0           dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 23640385\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.6838    \n",
      "Epoch 2/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.5363    \n",
      "Epoch 3/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.4140    \n",
      "Epoch 4/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.2814    \n",
      "Epoch 5/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.1672    \n",
      "Epoch 6/30\n",
      "19850/19850 [==============================] - 46s - loss: 2.0724    \n",
      "Epoch 7/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.9582    \n",
      "Epoch 8/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.8665    \n",
      "Epoch 9/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.7760    \n",
      "Epoch 10/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.6919    \n",
      "Epoch 11/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.6226    \n",
      "Epoch 12/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.5386    \n",
      "Epoch 13/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.4755    \n",
      "Epoch 14/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.4086    \n",
      "Epoch 15/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.3397    \n",
      "Epoch 16/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.2949    \n",
      "Epoch 17/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.2391    \n",
      "Epoch 18/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.1793    \n",
      "Epoch 19/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.1389    \n",
      "Epoch 20/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.0860    \n",
      "Epoch 21/30\n",
      "19850/19850 [==============================] - 46s - loss: 1.0409    \n",
      "Epoch 22/30\n",
      "19850/19850 [==============================] - 46s - loss: 0.9972    \n",
      "Epoch 23/30\n",
      "19850/19850 [==============================] - 46s - loss: 0.9642    \n",
      "Epoch 24/30\n",
      "19850/19850 [==============================] - 46s - loss: 0.9233    \n",
      "Epoch 25/30\n",
      "19850/19850 [==============================] - 46s - loss: 0.8909    \n",
      "Epoch 26/30\n",
      "19850/19850 [==============================] - 46s - loss: 0.8602    \n",
      "Epoch 27/30\n",
      "19850/19850 [==============================] - 47s - loss: 0.8168    \n",
      "Epoch 28/30\n",
      "19850/19850 [==============================] - 48s - loss: 0.7891    \n",
      "Epoch 29/30\n",
      "19850/19850 [==============================] - 48s - loss: 0.7654    \n",
      "Epoch 30/30\n",
      "19850/19850 [==============================] - 48s - loss: 0.7384    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7fba6eed0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] weights.hdf5 already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True in save_weights!\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8000, 11, 130, 63, 22, 126, 6, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "string = 'Are you going out with him to the'\n",
    "def convert(string):\n",
    "    word = nltk.word_tokenize(string)\n",
    "    word.insert(0,sentence_start_token)\n",
    "    word.append(sentence_end_token)\n",
    "    word = [word_to_index[w] if w in word_to_index else word_to_index[unknown_token] for w in word]\n",
    "    return word\n",
    "print convert(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[False  True False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False  True]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " ..., \n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]]\n",
      "\n",
      " [[False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  ..., \n",
      "  [False False False ..., False False False]\n",
      "  [False False False ..., False False False]\n",
      "  [False False  True ..., False False False]]]\n"
     ]
    }
   ],
   "source": [
    "word,i = convert(string),0\n",
    "X_test = np.zeros((len(word),maxlen,vocabulary_size+1),dtype = np.bool)\n",
    "for t,w in enumerate(word):\n",
    "    X_test[i,t,w] = 1\n",
    "    i += 1\n",
    "print X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    yPred = model.predict_classes(X_test,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[575   3  35 192   3   3   3   3   3   3]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print yPred\n",
    "print yPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code\n",
      ".\n",
      "?\n",
      "best\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in yPred:\n",
    "    print index_to_word[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
