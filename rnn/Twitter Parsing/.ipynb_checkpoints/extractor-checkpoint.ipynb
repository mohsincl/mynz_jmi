{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from operator import itemgetter\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convergence threshold is the maximum error in score convergence of TextRank\n",
    "CONVERGENCE_THRESHOLD = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of all nouns\n",
    "NOUNS = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    '''\n",
    "    The master class for our Document Summerization module.\n",
    "    Incorporates all features related to Document\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        #self.sents = sent_tokenize(self.document)\n",
    "        self.sents = self.document.split('\\n')\n",
    "        self.word_freq = FreqDist(clean(self.document))\n",
    "        self.graph = None\n",
    "        self.params = { 'thresh': 0.0 }\n",
    "                \n",
    "    def __str__(self):\n",
    "        return self.document\n",
    "    \n",
    "    \n",
    "    def statistical_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        Statistical similarity between sentences\n",
    "        based on the cosine method\n",
    "        Returns: float (the cosine similarity b/w sent1 and sent2)\n",
    "        '''\n",
    "        sent_token1 = Counter(sent1)\n",
    "        sent_token2 = Counter(sent2)\n",
    "        \n",
    "        intxn = set(sent_token1) & set(sent_token2)\n",
    "        numerator = sum([sent_token1[x] * sent_token2[x] for x in intxn])\n",
    "        \n",
    "        mod1 = sum([sent_token1[x]**2 for x in sent_token1.keys()])\n",
    "        mod2 = sum([sent_token2[x]**2 for x in sent_token2.keys()])\n",
    "        denominator = sqrt(mod1)*sqrt(mod2)\n",
    "        \n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "\n",
    "        return float(numerator)/denominator\n",
    "    \n",
    "    \n",
    "    def semantic_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        A semantic similarity score between two sentences\n",
    "        based on WordNet\n",
    "        Returns: float (the semantic similarity measure)\n",
    "        '''\n",
    "        score = 0\n",
    "        sent1 = [word for word in sent1 if word in NOUNS]\n",
    "        sent2 = [word for word in sent2 if word in NOUNS]\n",
    "        for t1 in sent1:\n",
    "            for t2 in sent2:\n",
    "                score += semantic_score(t1,t2)\n",
    "        try:\n",
    "            return score/(len(sent1 + sent2))  \n",
    "        except:\n",
    "            return 10000\n",
    "    \n",
    "    \n",
    "    def construct_graph(self):\n",
    "        '''\n",
    "        Constructs the word similarity graph\n",
    "        '''\n",
    "        connected = []\n",
    "        for pair in combinations(self.sents, 2):\n",
    "            cpair = clean(pair[0]), clean(pair[1])\n",
    "            weight = self.statistical_sim(*cpair) + \\\n",
    "                     self.semantic_sim(*cpair)\n",
    "            connected.append((pair[0], pair[1], weight))\n",
    "        self.graph = draw_graph(connected, self.params['thresh'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def clean(sent):\n",
    "    '''\n",
    "    A utility function that returns a a list of words in a sentence\n",
    "    after cleaning it. Gets rid off uppper-case, punctuations, \n",
    "    stop words, etc.\n",
    "    Returns: list (a list of cleaned words in sentence)\n",
    "    '''\n",
    "    words =  sent.lower() \n",
    "    words = re.findall(r'\\w+', words,flags = re.UNICODE | re.LOCALE) \n",
    "    imp_words = filter(lambda x: x not in stopwords.words('english'), words)\n",
    "    return imp_words\n",
    "        \n",
    "def semantic_score(word1, word2):\n",
    "    '''\n",
    "    Semantic score between two words based on WordNet\n",
    "    Returns: float (the semantic score between word1 and word2)\n",
    "    '''\n",
    "    try:\n",
    "        w1 = wn.synset('%s.n.01'%(word1))\n",
    "        w2 = wn.synset('%s.n.01'%(word2))\n",
    "        return wn.path_similarity(w1,w2,simulate_root = False)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def draw_graph(connected, thresh):\n",
    "    '''\n",
    "    Draws graph as per weights and puts edges if \n",
    "    weight exceed the given thresh\n",
    "    Returns: networkx Graph (nodes are sentences and edges\n",
    "             are statistical and semantic relationships)\n",
    "    '''\n",
    "    nodes = set([n1 for n1, n2, n3 in connected] + \\\n",
    "                [n2 for n1, n2, n3 in connected])\n",
    "    G=nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "    for edge in connected:\n",
    "        if edge[2] > thresh:\n",
    "            G.add_edge(edge[0], edge[1],weight = edge[2])\n",
    "    #plt.figure(figsize=(8,8))\n",
    "    #pos = nx.spring_layout(G)\n",
    "    #nx.draw(G,node_color='#A0CBE2', edge_color='orange',width=1,with_labels=False)\n",
    "    #plt.show()\n",
    "    return G\n",
    "    \n",
    "def textrank_weighted(graph, initial_value=None, damping=0.85):\n",
    "    '''\n",
    "    Calculates PageRank for an undirected graph\n",
    "    Returns: A list of tuples representing sentences and respective\n",
    "    scores in descending order\n",
    "    '''\n",
    "    if initial_value == None:\n",
    "        initial_value = 1.0 / len(graph.nodes())\n",
    "    scores = dict.fromkeys(graph.nodes(), initial_value)\n",
    "\n",
    "    iteration_quantity = 0\n",
    "    for iteration_number in xrange(100):\n",
    "        iteration_quantity += 1\n",
    "        convergence_achieved = 0\n",
    "        for i in graph.nodes():\n",
    "            rank = 1 - damping\n",
    "            for j in graph.neighbors(i):\n",
    "                neighbors_sum = sum([graph.get_edge_data(j, k)['weight'] for k in graph.neighbors(j)])\n",
    "                rank += damping * scores[j] * graph.get_edge_data(j, i)['weight'] / neighbors_sum\n",
    "\n",
    "            if abs(scores[i] - rank) <= CONVERGENCE_THRESHOLD:\n",
    "                convergence_achieved += 1\n",
    "\n",
    "            scores[i] = rank\n",
    "\n",
    "        if convergence_achieved == len(graph.nodes()):\n",
    "            break\n",
    "    return sorted(scores.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    return ' '.join(re.sub(\"(RT @[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|@[A-Za-z0-9]+\",\" \",tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_id(tweets,tag,tweet):\n",
    "    for t in tweets[tag]:\n",
    "        if tweet == (t[0]):\n",
    "            return t[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Najeeb Khan Credentials\n",
    "consumer_key = '3cFJ5hLMswDJOwXfaJGS4eNyS'\n",
    "consumer_secret = 'GRn1itnKbrCdAvHhbpjFgzWXMHePNuhDYz5scjnyhD8fC3Bnqg'\n",
    "access_token = '277893116-tcAWnc62SVdSBLUIqF5R5h92qm2Y0epfQUQJNa4l'\n",
    "access_secret = 'S47f26iaI7L0Z5AyT2NQqtsmitrMn70nZG2H5n5dyqM4C'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)\n",
    "trend = api.trends_place(23424977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trends(trend):\n",
    "    return [(i['name']) for i in trend[0]['trends']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'#GrowingUpWithLenientParents', u'#dotheboogaloo', u'#HeterosexualPrideDay', u'#BeingAsian', u'#BadMonsterMovies', u'Scotty Moore', u'Tyler Miller', u'Joe Maddon', u'Ryan Madson', u'Candace Parker', u'Brandon Crawford', u'Doolittle', u'4 Levels To Party', u'Javier Lopez', u'Project X', u'Bochy', u'Tim Lincecum', u'Eddie Butler', u'Arizona 5-4', u'Julio Urias', u'Michy Batshuayi', u'Melvin Upton', u'Nelson Cruz', u'Toronto-area', u'#lovesickmemories', u'#ThingsIWillAlwaysBe', u'#EthansHair', u'#MakeAmericaHornyAgain', u'#MTVScream', u'#GrowingUpHispanic', u'#WednesdayWisdom', u'#WhyIDontGetInvitedAnymore', u'#CheerGirlsBreakTheInternet', u'#pettyhour', u'#DeleteBeforeSunrise', u'#ThankYouLaurenFor', u'#BayBridgeSeries', u'#electionin6words', u'#SocialSoiree', u'#ZackAttack', u'#ZooCBS', u'#1lineWed', u'#ALDUBIYAMin14Days', u'#FinFabulousBET', u'#NoEraPenal', u'#ToTellTheTruth', u'#weirdbaseball', u'#MiercolesDeGanarSeguidores', u'#BLACKPINK', u'#TourGroup']\n"
     ]
    }
   ],
   "source": [
    "hashtags = get_trends(trend)\n",
    "print hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(hashtags = [],nb_tweets = 10):\n",
    "    tweets = {}\n",
    "    for tag in hashtags[0:10]:\n",
    "        Tweets = tweepy.Cursor(api.search, q=tag).items(10)\n",
    "        text = [(tweet_cleaner((tweet.text).encode('ascii','ignore')) + '.',tweet.id) for tweet in Tweets]\n",
    "        tweets[tag] = text\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = get_tweets(hashtags,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyler Miller\n",
      "----------------------------------------------------------\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748106487571025921)\n",
      "('Tyler Miller tymiller01 is now trending in Seattle.', 748104468776902657)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748103576942051330)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748101111995830272)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748096606126301184)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748095242478424064)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748092009164922881)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748078928082804736)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748078651946610688)\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748078148168744961)\n",
      "===========================================================\n",
      "Candace Parker\n",
      "----------------------------------------------------------\n",
      "('Awesome emotional interview between Holly Rowe and Candace Parker speaking on Pat Summitt.', 748110757817163776)\n",
      "('Watch Candace Parker give a very emotional post game interview about Pat Summitt.', 748110754902073344)\n",
      "('Candace Parker s postgame interview with Holly Rowe about Pat Summitt.', 748110742516203520)\n",
      "('WNBA Video Candace Parker Opens Up About Pat Summitt in Postgame Interview From here on out we re cont.', 748110704301973504)\n",
      "('Watch Candace Parker give a very emotional post game interview about Pat Summitt.', 748110606637596672)\n",
      "('Candace Parker s postgame interview with Holly Rowe about Pat Summitt.', 748110584445607937)\n",
      "('First Candace Parker is left out of the USA team and now Abby Knight10 is left out of ours What on earth is going on Rio2016.', 748110576237187072)\n",
      "('Candace Parker scored 31pts 13rebs 7asts and put on a show tonight in the 2nd Half reminding us why she s the best pla.', 748110526203334656)\n",
      "('Candace Parker says she is playing for Pat Summitt from here onout.', 748110250746667008)\n",
      "('Candace Parker s postgame interview with Holly Rowe about Pat Summitt.', 748110184283766784)\n",
      "===========================================================\n",
      "#HeterosexualPrideDay\n",
      "----------------------------------------------------------\n",
      "('Based off the outrage of HeterosexualPrideDay it appears everyone on the left doesn t support diversity and equality.', 748110677726867460)\n",
      "('Les htros vous tes contre nature allez tre des dgnrs mentaux ailleurs svp HeterosexualPrideDay.', 748110677684916225)\n",
      "('HeterosexualPrideDay So we don t become evolutionary dead ends.', 748110677118754817)\n",
      "('Dopo l HeterosexualPrideDay cosa dobbiamo aspettarci La giornata sui nazisti orgogliosi Vabb che di questi tempi.', 748110675961077760)\n",
      "('HeterosexualPrideDay.', 748110675109679104)\n",
      "('NEOBRUTALIST until we live in a heterophobic rather than homophobic society HeterosexualPrideDay will remain counterproductive to society.', 748110674933456896)\n",
      "('I fucking hate y all HeterosexualPrideDay.', 748110674375618561)\n",
      "('En rouge les pays o l htrosexualit est un crime HeterosexualPrideDay.', 748110673981353984)\n",
      "('Isn t HeterosexualPrideDay everyday Walking around freely without the fear of being attacked or killed because of your.', 748110673817772032)\n",
      "('Valentina HeterosexualPrideDay Por todos lxs heterosexuales que han sufrido agresiones en Madrid este ao solo por su orient.', 748110673465479168)\n",
      "===========================================================\n",
      "Joe Maddon\n",
      "----------------------------------------------------------\n",
      "('Hey LouMerloni thoughts on the moves and staff management by Joe Maddon last night Do we have any of that kind of thinking in Boston.', 748108804026470400)\n",
      "('Corey Crawford Q it s me Want to run an idea Joel Quenneville You are NOT playing left wing for a shift C.', 748108211815751680)\n",
      "('Joe Maddon is playing four dimensional chess while everyone else is playing checkers.', 748107961642319873)\n",
      "('Joe maddon is a sorcerer.', 748107399853015040)\n",
      "('Do not try to out manage Joe Maddon.', 748107314733912064)\n",
      "('Joe Maddon living out Joe Maddon fanfic IRL tonight.', 748107256261009408)\n",
      "('joe maddon is the best.', 748107062886858752)\n",
      "('Fell asleep before the cubs game ended and just now looking at what the mad genius Joe Maddon did late in the game.', 748106258209607682)\n",
      "('I like Joe Maddon but can we calm down Whitey Herzog several times shuffled Todd Worrell from RF to P in the mid 80s.', 748105883834449920)\n",
      "('Joe Maddon with the Ol Waxahachie Swap Classic.', 748105748882690048)\n",
      "===========================================================\n",
      "Scotty Moore\n",
      "----------------------------------------------------------\n",
      "('I m so very sad to hear of Scotty Moore s passing He electrified a generation Elvis guitarist dies aged 84.', 748110738527399936)\n",
      "('Scotty Moore with officialKeef RIPScottyMoore.', 748110695653314560)\n",
      "('Everyone wanted to be Elvis I wanted to be Scotty Moore Keith Richards RIPScottyMoore.', 748110652762296321)\n",
      "('RIP Scotty Moore Elvis guitarist who has passed at 84.', 748110640775036928)\n",
      "('Scotty Moore Rock Pioneer and Elvis Presleys Guitarist Dies at 84.', 748110640481329152)\n",
      "('Scotty Moore hard driving guitarist who backed Elvis Presley dies at 84.', 748110639193718784)\n",
      "('Scotty Moore RIP and Elvis.', 748110611603664896)\n",
      "('ScottyMoore who helped ElvisPresley shape his revolutionary sound dies at 84.', 748110584193908736)\n",
      "('Elvis guitarist Scotty Moore dies aged 84 BBC News.', 748110547355242501)\n",
      "('ElvisRadio19 My sincere condolences to the family of Scotty Moore and everyone at ElvisRadio19 on his sudden passing Truly 1 of the best.', 748110537255493632)\n",
      "===========================================================\n",
      "Ryan Madson\n",
      "----------------------------------------------------------\n",
      "('Bumpy last innings in SF finally tip to A s sanfrancisco.', 748106218229612545)\n",
      "('Ryan Madson hits left handed.', 748102786743734272)\n",
      "('Josh Phegley s two run double gives the A s an 12 9 lead in the ninth Now Ryan Madson is hitting or I m seeing things.', 748098240508108800)\n",
      "('Ryan Madson is the first pitcher to give up three runs and earn a save in less than three innings since Carlos Marmol in 2010.', 748097807785943040)\n",
      "('Ryan Madson Bumpy last innings in SF finally tip to A s MLB.', 748096128869105664)\n",
      "('Athletics Bumpy last innings in SF finally tip to A s.', 748094721462046720)\n",
      "('wp MLB Bumpy last innings in SF finally tip to A s.', 748093419705237504)\n",
      "('A s Ryan Madson gets 2 inning save vs Giants.', 748093019501453312)\n",
      "('Bumpy last innings in SF finally tip to As Oakland closer Ryan Madson was a central figure T MLB baseball.', 748090073288105984)\n",
      "('wp MLB Bumpy last innings in SF finally tip to A s.', 748089230056886272)\n",
      "===========================================================\n",
      "#dotheboogaloo\n",
      "----------------------------------------------------------\n",
      "('lets dotheboogaloo.', 748110715307831296)\n",
      "('Ashton robi solow karier No moe zatrudni Caluma jako operatora kamery telefonu dotheboogaloo.', 748110714632609792)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110714590531584)\n",
      "('dotheboogaloo got me like.', 748110712468180992)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110710966730752)\n",
      "('In 30 seconds Ashton made my whole day DoTheBoogaloo.', 748110710102724608)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110706822811648)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110706751438848)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110706453647360)\n",
      "('Are we gonna get this trending dotheboogaloo.', 748110706126487556)\n",
      "===========================================================\n",
      "#BadMonsterMovies\n",
      "----------------------------------------------------------\n",
      "('Not Feratu BadMonsterMovies.', 748110633367810048)\n",
      "('Cloverfield of Dreams BadMonsterMovies midnight.', 748110582927134720)\n",
      "('National Lampoon s Animal House of Dracula BadMonsterMovies.', 748110498571423744)\n",
      "('Trump vs Hillary BadMonsterMovies.', 748110452035452928)\n",
      "('Godzilla Vs The Board Of Education BadMonsterMovies midnight.', 748110425653317632)\n",
      "('The Godzillas must be crazy BadMonsterMovies midnight.', 748110405801648130)\n",
      "('The new Ghostbusters BadMonsterMovies.', 748110347853144064)\n",
      "('Pacific Rim Job BadMonsterMovies midnight.', 748110337799401473)\n",
      "('Arctic Rim Kaiju On Ice BadMonsterMovies.', 748110262922776577)\n",
      "('AllyPayne ays Pepsi s going back to aspartame cause too many people walked the plank off the Pepsico parade hip BadMonsterMovies.', 748110262327193600)\n",
      "===========================================================\n",
      "#GrowingUpWithLenientParents\n",
      "----------------------------------------------------------\n",
      "('GrowingUpWithLenientParents when your friends told you they got grounded but you couldn t relate.', 748110706483036160)\n",
      "('PH Trending Philippines 06 19 PM PHT 5 GrowingUpWithLenientParents 6 WALANG PASOK BUKAS 7 JaneOineza MMKsaSabado 8 Suprem.', 748110706478710784)\n",
      "('GrowingUpWithLenientParents telling your dad you re leaving as you re walking out the house and all he says is okay be.', 748110705367220224)\n",
      "('When their lenient yet strict and it confuses you because you never know when they ll switch it up on ya growingupwith.', 748110705342058496)\n",
      "('GrowingUpWithLenientParents having a good trusting relationship where they let you go out and have fun because they know.', 748110700904579072)\n",
      "('GrowingUpWithLenientParents when all your friends complain about their strict parents and you re just like.', 748110694567055360)\n",
      "('GrowingUpWithLenientParents never been grounded a day in my life.', 748110693493317632)\n",
      "('GrowingUpWithLenientParents ok lock the door when you get home goodnight.', 748110691513606144)\n",
      "('I relate to both GrowingUpWithStrictParents and GrowingUpWithLenientParents bc my mom randomly be switching up on me lmao.', 748110691098312705)\n",
      "('when you can t relate to the GrowingUpWithLenientParents tag because you grew up with strict parents.', 748110687990276096)\n",
      "===========================================================\n",
      "#BeingAsian\n",
      "----------------------------------------------------------\n",
      "('beingasian is having ppl tell u ur food smells bad ur language sounds funny but saying ur culture is awesome when it fit.', 748110733477445633)\n",
      "('no we don t eat dogs or cats beingasian.', 748110728545042432)\n",
      "('BeingAsian having ur peers mock the nail salon ladies accent when she at least learned English amp theyre too dumb to learn an.', 748110725143396353)\n",
      "('BeingAsian in Singapore and being brown immediately Malay no I don t have any Malay heritage.', 748110718386331648)\n",
      "('asian grading system A average B bad C can t eat dinner D don t come home F find another family BeingAsian.', 748110716314353668)\n",
      "('BeingAsian calling every Filipino person your Tito Tita or Ate even if you re not related to them.', 748110715085459457)\n",
      "('BeingAsian who are you going out with what time are you going home i need their mom s numbers i need the exact loca.', 748110710673051648)\n",
      "('asian grading system A average B bad C can t eat dinner D don t come home F find another family BeingAsian.', 748110708089335809)\n",
      "('asian grading system A average B bad C can t eat dinner D don t come home F find another family BeingAsian.', 748110700015345664)\n",
      "('asian grading system A average B bad C can t eat dinner D don t come home F find another family BeingAsian.', 748110694462033920)\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "for tag in tweets:\n",
    "    print tag\n",
    "    print '----------------------------------------------------------'\n",
    "    for tweet in tweets[tag]:\n",
    "        print tweet\n",
    "    print '==========================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_tweets(tweets = {},top = 3):\n",
    "    extract_tweets = {}\n",
    "    for tag in tweets:\n",
    "        temp = [tweet[0] for tweet in tweets[tag]]\n",
    "        string = '\\n'.join(temp)\n",
    "        a = Document(string)\n",
    "        a.construct_graph()\n",
    "        x = textrank_weighted(a.graph)\n",
    "        extract_tweets[tag] = [(i[0],find_id(tweets,tag,i[0])) for i in x[:top]]\n",
    "    return extract_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract = get_top_tweets(tweets,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyler Miller\n",
      "('TYLER FREAKIN MILLER Enjoy the penalty shootout in its entirety.', 748106487571025921)\n",
      "('Tyler Miller tymiller01 is now trending in Seattle.', 748104468776902657)\n",
      "Candace Parker\n",
      "('Candace Parker s postgame interview with Holly Rowe about Pat Summitt.', 748110742516203520)\n",
      "('Watch Candace Parker give a very emotional post game interview about Pat Summitt.', 748110754902073344)\n",
      "('Awesome emotional interview between Holly Rowe and Candace Parker speaking on Pat Summitt.', 748110757817163776)\n",
      "#HeterosexualPrideDay\n",
      "('HeterosexualPrideDay.', 748110675109679104)\n",
      "('I fucking hate y all HeterosexualPrideDay.', 748110674375618561)\n",
      "('En rouge les pays o l htrosexualit est un crime HeterosexualPrideDay.', 748110673981353984)\n",
      "Joe Maddon\n",
      "('Joe Maddon living out Joe Maddon fanfic IRL tonight.', 748107256261009408)\n",
      "('joe maddon is the best.', 748107062886858752)\n",
      "('Joe maddon is a sorcerer.', 748107399853015040)\n",
      "Scotty Moore\n",
      "('RIP Scotty Moore Elvis guitarist who has passed at 84.', 748110640775036928)\n",
      "('Elvis guitarist Scotty Moore dies aged 84 BBC News.', 748110547355242501)\n",
      "('Scotty Moore Rock Pioneer and Elvis Presleys Guitarist Dies at 84.', 748110640481329152)\n",
      "Ryan Madson\n",
      "('wp MLB Bumpy last innings in SF finally tip to A s.', 748093419705237504)\n",
      "('Ryan Madson Bumpy last innings in SF finally tip to A s MLB.', 748096128869105664)\n",
      "('Bumpy last innings in SF finally tip to As Oakland closer Ryan Madson was a central figure T MLB baseball.', 748090073288105984)\n",
      "#dotheboogaloo\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110714590531584)\n",
      "('In 30 seconds Ashton made my whole day DoTheBoogaloo.', 748110710102724608)\n",
      "('lets dotheboogaloo.', 748110715307831296)\n",
      "#BadMonsterMovies\n",
      "('Not Feratu BadMonsterMovies.', 748110633367810048)\n",
      "('The new Ghostbusters BadMonsterMovies.', 748110347853144064)\n",
      "('Pacific Rim Job BadMonsterMovies midnight.', 748110337799401473)\n",
      "#GrowingUpWithLenientParents\n",
      "('GrowingUpWithLenientParents when your friends told you they got grounded but you couldn t relate.', 748110706483036160)\n",
      "('I relate to both GrowingUpWithStrictParents and GrowingUpWithLenientParents bc my mom randomly be switching up on me lmao.', 748110691098312705)\n",
      "('GrowingUpWithLenientParents having a good trusting relationship where they let you go out and have fun because they know.', 748110700904579072)\n",
      "#BeingAsian\n",
      "('asian grading system A average B bad C can t eat dinner D don t come home F find another family BeingAsian.', 748110716314353668)\n",
      "('beingasian is having ppl tell u ur food smells bad ur language sounds funny but saying ur culture is awesome when it fit.', 748110733477445633)\n",
      "('BeingAsian having ur peers mock the nail salon ladies accent when she at least learned English amp theyre too dumb to learn an.', 748110725143396353)\n"
     ]
    }
   ],
   "source": [
    "for tag in extract:\n",
    "    print tag\n",
    "    for tweet in extract[tag]:\n",
    "        print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = get_top_tweets(extract,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tyler Miller tymiller01 is now trending in Seattle.', 748104468776902657)\n",
      "('Awesome emotional interview between Holly Rowe and Candace Parker speaking on Pat Summitt.', 748110757817163776)\n",
      "('HeterosexualPrideDay.', 748110675109679104)\n",
      "('Joe Maddon living out Joe Maddon fanfic IRL tonight.', 748107256261009408)\n",
      "('Scotty Moore Rock Pioneer and Elvis Presleys Guitarist Dies at 84.', 748110640481329152)\n",
      "('Ryan Madson Bumpy last innings in SF finally tip to A s MLB.', 748096128869105664)\n",
      "('Here s an original song for you guys Enjoy dotheboogaloo.', 748110714590531584)\n",
      "('Not Feratu BadMonsterMovies.', 748110633367810048)\n",
      "('GrowingUpWithLenientParents when your friends told you they got grounded but you couldn t relate.', 748110706483036160)\n",
      "('beingasian is having ppl tell u ur food smells bad ur language sounds funny but saying ur culture is awesome when it fit.', 748110733477445633)\n"
     ]
    }
   ],
   "source": [
    "for tag in final:\n",
    "    for tweet in final[tag]:\n",
    "        print tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
